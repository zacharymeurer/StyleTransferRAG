{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d362c0d7-b173-4a5d-b0e0-6b1c87a4da80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c5645b84-0b32-42f2-99a3-a567837683e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Encoder\n",
    "encoder = models.vgg19(weights='DEFAULT')\n",
    "\n",
    "#Decoder\n",
    "decoder = nn.Sequential(\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(512, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 256, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(256, 128, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 128, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(128, 64, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.Upsample(scale_factor=2, mode='nearest'),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 64, (3, 3)),\n",
    "    nn.ReLU(),\n",
    "    nn.ReflectionPad2d((1, 1, 1, 1)),\n",
    "    nn.Conv2d(64, 3, (3, 3)),\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "98e52726-59c3-470f-999a-2c7af64a4f81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.features[4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f2f8ac28-52d5-4cbe-b3e7-ea72864e4f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Sequential(\n",
       "  (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (1): ReLU(inplace=True)\n",
       "  (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "  (3): ReLU(inplace=True)\n",
       "  (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder.features[:6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fcbf073f-df26-4d6f-b65a-0fed1f563cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions\n",
    "def calc_mean_std(feat, eps=1e-5):\n",
    "    # eps is a small value added to the variance to avoid divide-by-zero.\n",
    "    size = feat.size()\n",
    "    assert (len(size) == 4)\n",
    "    N, C = size[:2]\n",
    "    feat_var = feat.view(N, C, -1).var(dim=2) + eps\n",
    "    feat_std = feat_var.sqrt().view(N, C, 1, 1)\n",
    "    feat_mean = feat.view(N, C, -1).mean(dim=2).view(N, C, 1, 1)\n",
    "    return feat_mean, feat_std\n",
    "\n",
    "def adaptive_instance_normalization(content_feat, style_feat):\n",
    "    assert (content_feat.size()[:2] == style_feat.size()[:2])\n",
    "    size = content_feat.size()\n",
    "    style_mean, style_std = calc_mean_std(style_feat)\n",
    "    content_mean, content_std = calc_mean_std(content_feat)\n",
    "\n",
    "    normalized_feat = (content_feat - content_mean.expand(\n",
    "        size)) / content_std.expand(size)\n",
    "    return normalized_feat * style_std.expand(size) + style_mean.expand(size)\n",
    "\n",
    "# Sampler Functions\n",
    "import numpy as np\n",
    "from torch.utils import data\n",
    "\n",
    "def InfiniteSampler(n):\n",
    "    # i = 0\n",
    "    i = n - 1\n",
    "    order = np.random.permutation(n)\n",
    "    while True:\n",
    "        yield order[i]\n",
    "        i += 1\n",
    "        if i >= n:\n",
    "            np.random.seed()\n",
    "            order = np.random.permutation(n)\n",
    "            i = 0\n",
    "\n",
    "class InfiniteSamplerWrapper(data.sampler.Sampler):\n",
    "    def __init__(self, data_source):\n",
    "        self.num_samples = len(data_source)\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter(InfiniteSampler(self.num_samples))\n",
    "\n",
    "    def __len__(self):\n",
    "        return 2 ** 31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edd9531a-ab3b-4ff4-88c2-ed1c660e3553",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Adain Network\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Net, self).__init__()\n",
    "        enc_layers = list(encoder.children()) # relus setup for default vgg19 (if this causes issues, change setup to custom vgg)\n",
    "        self.enc_1 = nn.Sequential(*enc_layers[:1])  # input -> relu1_1\n",
    "        self.enc_2 = nn.Sequential(*enc_layers[1:6])  # relu1_1 -> relu2_1\n",
    "        self.enc_3 = nn.Sequential(*enc_layers[6:11])  # relu2_1 -> relu3_1\n",
    "        self.enc_4 = nn.Sequential(*enc_layers[11:20])  # relu3_1 -> relu4_1\n",
    "        self.decoder = decoder\n",
    "        self.mse_loss = nn.MSELoss()\n",
    "\n",
    "        # fix the encoder\n",
    "        for name in ['enc_1', 'enc_2', 'enc_3', 'enc_4']:\n",
    "            for param in getattr(self, name).parameters():\n",
    "                param.requires_grad = False\n",
    "\n",
    "    # extract relu1_1, relu2_1, relu3_1, relu4_1 from input image\n",
    "    def encode_with_intermediate(self, input):\n",
    "        results = [input]\n",
    "        for i in range(4):\n",
    "            func = getattr(self, 'enc_{:d}'.format(i + 1))\n",
    "            results.append(func(results[-1]))\n",
    "        return results[1:]\n",
    "\n",
    "    # extract relu4_1 from input image\n",
    "    def encode(self, input):\n",
    "        for i in range(4):\n",
    "            input = getattr(self, 'enc_{:d}'.format(i + 1))(input)\n",
    "        return input\n",
    "\n",
    "    def calc_content_loss(self, input, target):\n",
    "        assert (input.size() == target.size())\n",
    "        assert (target.requires_grad is False)\n",
    "        return self.mse_loss(input, target)\n",
    "\n",
    "    def calc_style_loss(self, input, target):\n",
    "        assert (input.size() == target.size())\n",
    "        assert (target.requires_grad is False)\n",
    "        input_mean, input_std = calc_mean_std(input)\n",
    "        target_mean, target_std = calc_mean_std(target)\n",
    "        return self.mse_loss(input_mean, target_mean) + \\\n",
    "               self.mse_loss(input_std, target_std)\n",
    "\n",
    "    def forward(self, content, style, alpha=1.0):\n",
    "        assert 0 <= alpha <= 1\n",
    "        style_feats = self.encode_with_intermediate(style)\n",
    "        content_feat = self.encode(content)\n",
    "        t = adain(content_feat, style_feats[-1])\n",
    "        t = alpha * t + (1 - alpha) * content_feat\n",
    "\n",
    "        g_t = self.decoder(t)\n",
    "        g_t_feats = self.encode_with_intermediate(g_t)\n",
    "\n",
    "        loss_c = self.calc_content_loss(g_t_feats[-1], t)\n",
    "        loss_s = self.calc_style_loss(g_t_feats[0], style_feats[0])\n",
    "        for i in range(1, 4):\n",
    "            loss_s += self.calc_style_loss(g_t_feats[i], style_feats[i])\n",
    "        return loss_c, loss_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fae768f-a511-4340-a232-9cf0367d1fa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Unmodified\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from PIL import Image, ImageFile\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "import net\n",
    "from sampler import InfiniteSamplerWrapper\n",
    "\n",
    "cudnn.benchmark = True\n",
    "Image.MAX_IMAGE_PIXELS = None  # Disable DecompressionBombError\n",
    "# Disable OSError: image file is truncated\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "def train_transform():\n",
    "    transform_list = [\n",
    "        transforms.Resize(size=(512, 512)),\n",
    "        transforms.RandomCrop(256),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "\n",
    "class FlatFolderDataset(data.Dataset): # Looks good\n",
    "    def __init__(self, root, transform):\n",
    "        super(FlatFolderDataset, self).__init__()\n",
    "        self.root = root\n",
    "        self.paths = list(Path(self.root).glob('*'))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        img = Image.open(str(path)).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def name(self):\n",
    "        return 'FlatFolderDataset'\n",
    "\n",
    "my_lr = 1e-4\n",
    "my_lr_decay = 5e-5\n",
    "\n",
    "def adjust_learning_rate(optimizer, iteration_count): # Looks good\n",
    "    \"\"\"Imitating the original implementation\"\"\"\n",
    "    lr = my_lr / (1.0 + my_lr_decay * iteration_count)\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# Basic options\n",
    "parser.add_argument('--content_dir', type=str, required=True,\n",
    "                    help='Directory path to a batch of content images')\n",
    "parser.add_argument('--style_dir', type=str, required=True,\n",
    "                    help='Directory path to a batch of style images')\n",
    "parser.add_argument('--vgg', type=str, default='models/vgg_normalised.pth')\n",
    "\n",
    "# training options\n",
    "parser.add_argument('--save_dir', default='./experiments',\n",
    "                    help='Directory to save the model')\n",
    "parser.add_argument('--log_dir', default='./logs',\n",
    "                    help='Directory to save the log')\n",
    "parser.add_argument('--max_iter', type=int, default=160000)\n",
    "parser.add_argument('--batch_size', type=int, default=8)\n",
    "parser.add_argument('--style_weight', type=float, default=10.0)\n",
    "parser.add_argument('--content_weight', type=float, default=1.0)\n",
    "parser.add_argument('--n_threads', type=int, default=16)\n",
    "parser.add_argument('--save_model_interval', type=int, default=10000)\n",
    "args = parser.parse_args()\n",
    "\n",
    "my_save_dir = './experiments'\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "save_dir = Path(my_save_dir) # CHANGE ARGS\n",
    "save_dir.mkdir(exist_ok=True, parents=True)\n",
    "log_dir = Path(args.log_dir) # CHANGE ARGS\n",
    "log_dir.mkdir(exist_ok=True, parents=True)\n",
    "writer = SummaryWriter(log_dir=str(log_dir)) # Optional\n",
    "\n",
    "decoder = net.decoder # CHANGE unnecessary since it's written in code box above\n",
    "vgg = net.vgg # CHANGE unnecessary since it's written in code box above\n",
    "\n",
    "vgg.load_state_dict(torch.load(args.vgg)) # CHANGE unnecessary since using pretrained vgg19\n",
    "vgg = nn.Sequential(*list(vgg.children())[:21]) # changed from 31 to 21\n",
    "network = net.Net(vgg, decoder) # CHANGE don't need net.\n",
    "network.train()\n",
    "network.to(device)\n",
    "\n",
    "content_dataset = FlatFolderDataset(args.content_dir, train_transform()) # CHANGE ARGS\n",
    "style_dataset = FlatFolderDataset(args.style_dir, train_transform()) # CHANGE ARGS\n",
    "\n",
    "content_iter = iter(data.DataLoader( # Looks good\n",
    "    content_dataset, batch_size=args.batch_size,\n",
    "    sampler=InfiniteSamplerWrapper(content_dataset),\n",
    "    num_workers=args.n_threads))\n",
    "style_iter = iter(data.DataLoader( # Looks good\n",
    "    style_dataset, batch_size=args.batch_size,\n",
    "    sampler=InfiniteSamplerWrapper(style_dataset),\n",
    "    num_workers=args.n_threads))\n",
    "\n",
    "optimizer = torch.optim.Adam(network.decoder.parameters(), lr=args.lr)\n",
    "\n",
    "for i in tqdm(range(args.max_iter)):\n",
    "    adjust_learning_rate(optimizer, iteration_count=i)\n",
    "    content_images = next(content_iter).to(device)\n",
    "    style_images = next(style_iter).to(device)\n",
    "    loss_c, loss_s = network(content_images, style_images)\n",
    "    loss_c = args.content_weight * loss_c # CHANGE ARGS\n",
    "    loss_s = args.style_weight * loss_s # CHANGE ARGS\n",
    "    loss = loss_c + loss_s\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    writer.add_scalar('loss_content', loss_c.item(), i + 1)\n",
    "    writer.add_scalar('loss_style', loss_s.item(), i + 1)\n",
    "\n",
    "    if (i + 1) % args.save_model_interval == 0 or (i + 1) == args.max_iter: # CHANGE ARGS\n",
    "        state_dict = net.decoder.state_dict()\n",
    "        for key in state_dict.keys():\n",
    "            state_dict[key] = state_dict[key].to(torch.device('cpu'))\n",
    "        torch.save(state_dict, save_dir /\n",
    "                   'decoder_iter_{:d}.pth.tar'.format(i + 1))\n",
    "writer.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35f1c8fd-08d4-407f-bc93-9d6997f2513b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Training Unmodified\n",
    "import argparse\n",
    "from pathlib import Path\n",
    "\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as data\n",
    "from PIL import Image, ImageFile\n",
    "from tensorboardX import SummaryWriter\n",
    "from torchvision import transforms\n",
    "from tqdm import tqdm\n",
    "\n",
    "import net\n",
    "from sampler import InfiniteSamplerWrapper\n",
    "\n",
    "cudnn.benchmark = True\n",
    "Image.MAX_IMAGE_PIXELS = None  # Disable DecompressionBombError\n",
    "# Disable OSError: image file is truncated\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "\n",
    "def train_transform():\n",
    "    transform_list = [\n",
    "        transforms.Resize(size=(512, 512)),\n",
    "        transforms.RandomCrop(256),\n",
    "        transforms.ToTensor()\n",
    "    ]\n",
    "    return transforms.Compose(transform_list)\n",
    "\n",
    "\n",
    "class FlatFolderDataset(data.Dataset): # Looks good\n",
    "    def __init__(self, root, transform):\n",
    "        super(FlatFolderDataset, self).__init__()\n",
    "        self.root = root\n",
    "        self.paths = list(Path(self.root).glob('*'))\n",
    "        self.transform = transform\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        path = self.paths[index]\n",
    "        img = Image.open(str(path)).convert('RGB')\n",
    "        img = self.transform(img)\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def name(self):\n",
    "        return 'FlatFolderDataset'\n",
    "\n",
    "\n",
    "def adjust_learning_rate(optimizer, iteration_count):\n",
    "    \"\"\"Imitating the original implementation\"\"\"\n",
    "    lr = args.lr / (1.0 + args.lr_decay * iteration_count) # CHANGE ARGS\n",
    "    for param_group in optimizer.param_groups:\n",
    "        param_group['lr'] = lr\n",
    "\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "# Basic options\n",
    "parser.add_argument('--content_dir', type=str, required=True,\n",
    "                    help='Directory path to a batch of content images')\n",
    "parser.add_argument('--style_dir', type=str, required=True,\n",
    "                    help='Directory path to a batch of style images')\n",
    "parser.add_argument('--vgg', type=str, default='models/vgg_normalised.pth')\n",
    "\n",
    "# training options\n",
    "parser.add_argument('--save_dir', default='./experiments',\n",
    "                    help='Directory to save the model')\n",
    "parser.add_argument('--log_dir', default='./logs',\n",
    "                    help='Directory to save the log')\n",
    "parser.add_argument('--lr', type=float, default=1e-4)\n",
    "parser.add_argument('--lr_decay', type=float, default=5e-5)\n",
    "parser.add_argument('--max_iter', type=int, default=160000)\n",
    "parser.add_argument('--batch_size', type=int, default=8)\n",
    "parser.add_argument('--style_weight', type=float, default=10.0)\n",
    "parser.add_argument('--content_weight', type=float, default=1.0)\n",
    "parser.add_argument('--n_threads', type=int, default=16)\n",
    "parser.add_argument('--save_model_interval', type=int, default=10000)\n",
    "args = parser.parse_args()\n",
    "\n",
    "device = torch.device('cuda') # CHANGE if cuda available\n",
    "save_dir = Path(args.save_dir) # CHANGE ARGS\n",
    "save_dir.mkdir(exist_ok=True, parents=True)\n",
    "log_dir = Path(args.log_dir) # CHANGE ARGS\n",
    "log_dir.mkdir(exist_ok=True, parents=True)\n",
    "writer = SummaryWriter(log_dir=str(log_dir)) # Optional\n",
    "\n",
    "decoder = net.decoder # CHANGE unnecessary since it's written in code box above\n",
    "vgg = net.vgg # CHANGE unnecessary since it's written in code box above\n",
    "\n",
    "vgg.load_state_dict(torch.load(args.vgg)) # CHANGE unnecessary since using pretrained vgg19\n",
    "vgg = nn.Sequential(*list(vgg.children())[:21]) # changed from 31 to 21\n",
    "network = net.Net(vgg, decoder) # CHANGE don't need net.\n",
    "network.train()\n",
    "network.to(device)\n",
    "\n",
    "content_dataset = FlatFolderDataset(args.content_dir, train_transform()) # CHANGE ARGS\n",
    "style_dataset = FlatFolderDataset(args.style_dir, train_transform()) # CHANGE ARGS\n",
    "\n",
    "content_iter = iter(data.DataLoader( # Looks good\n",
    "    content_dataset, batch_size=args.batch_size,\n",
    "    sampler=InfiniteSamplerWrapper(content_dataset),\n",
    "    num_workers=args.n_threads))\n",
    "style_iter = iter(data.DataLoader( # Looks good\n",
    "    style_dataset, batch_size=args.batch_size,\n",
    "    sampler=InfiniteSamplerWrapper(style_dataset),\n",
    "    num_workers=args.n_threads))\n",
    "\n",
    "optimizer = torch.optim.Adam(network.decoder.parameters(), lr=args.lr)\n",
    "\n",
    "for i in tqdm(range(args.max_iter)):\n",
    "    adjust_learning_rate(optimizer, iteration_count=i)\n",
    "    content_images = next(content_iter).to(device)\n",
    "    style_images = next(style_iter).to(device)\n",
    "    loss_c, loss_s = network(content_images, style_images)\n",
    "    loss_c = args.content_weight * loss_c # CHANGE ARGS\n",
    "    loss_s = args.style_weight * loss_s # CHANGE ARGS\n",
    "    loss = loss_c + loss_s\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    writer.add_scalar('loss_content', loss_c.item(), i + 1)\n",
    "    writer.add_scalar('loss_style', loss_s.item(), i + 1)\n",
    "\n",
    "    if (i + 1) % args.save_model_interval == 0 or (i + 1) == args.max_iter: # CHANGE ARGS\n",
    "        state_dict = net.decoder.state_dict()\n",
    "        for key in state_dict.keys():\n",
    "            state_dict[key] = state_dict[key].to(torch.device('cpu'))\n",
    "        torch.save(state_dict, save_dir /\n",
    "                   'decoder_iter_{:d}.pth.tar'.format(i + 1))\n",
    "writer.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:dsml]",
   "language": "python",
   "name": "conda-env-dsml-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
